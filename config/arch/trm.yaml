name: recursive_reasoning.trm@TinyRecursiveReasoningModel_ACTV1
loss:
  name: losses@ACTLossHead
  loss_type: stablemax_cross_entropy

halt_exploration_prob: 0.1
halt_max_steps: 16

H_cycles: 3
L_cycles: 6

H_layers: 0
L_layers: 2

hidden_size: 512
num_heads: 8        # still used for RoPE dim split etc., though attention is replaced
expansion: 4

puzzle_emb_ndim: ${.hidden_size}

pos_encodings: rope
forward_dtype: bfloat16

mlp_t: False        # use mamba (not mlp_t) on L
puzzle_emb_len: 16
no_ACT_continue: True

# ðŸ”¹ Mamba hyperparameters
mamba_d_state: 64
mamba_d_conv: 1
mamba_expand: 4.0
mamba_dt_rank: auto

# ðŸ”¹ Our modes (exactly one of these should be true)
mamba_bimamba_v2_with_transformer: False
mamba_bimamba_with_transformer_and_nn: False
mamba_two_stage: False          # disable old two-stage design
mamba_bimamba_v2: False          # enable new BiMamba v2 block
mamba_if_divide_out: True       # match Vision Mamba's /2 behavior

mamba_bimamba_heads_transformer_nn: False
mamba_bimamba_heads: 8
mamba_bimamba_heads_out_proj: True  # optional mixing after concat (adds params)

bilstm_with_nn: True
bilstm_hidden_size: 512      # 0 => use hidden_size (per direction)
bilstm_num_layers: 1
bilstm_downproj_bias: False
